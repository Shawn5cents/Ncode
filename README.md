<div align="center">

# ğŸš€ Ncode v1.0.0

### AI-Powered Code Generation, Reimagined

[![License](https://img.shields.io/static/v1?label=license&message=MIT&color=blue)](LICENSE)
[![Python](https://img.shields.io/static/v1?label=python&message=3.10%2B&color=blue)](https://python.org)
[![Status](https://img.shields.io/static/v1?label=status&message=active&color=success)]()
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Contributing](#-contributing) â€¢ [License](#-license)

---

```
  _   _              _      
 | \ | |            | |     
 |  \| | ___ ___  __| | ___ 
 | . ` |/ __/ _ \/ _` |/ _ \
 | |\  | (_| (_) | (_| |  __/
 |_| \_|\___\___/ \__,_|\___|
                             
 Local Code Generation System
```

</div>

Transform your ideas into code with Ncode - a powerful local code generation system that combines LLaMA's intelligence with blazing-fast performance. Experience the future of coding with our unique dual-model approach: one for planning, one for implementation.

## âœ¨ Features

| Feature | Description | Benefits |
|---------|-------------|-----------|
| ğŸš„ **Multi-Token Prediction (MTP)** | Revolutionary parallel token generation | Up to 5x faster code generation |
| ğŸ”„ **Parallel Processing** | Simultaneous planning and implementation | Efficient workflow, better results |
| ğŸ¯ **GPU Acceleration** | Smart GPU layer optimization | Maximum performance on your hardware |
| ğŸ”§ **Model Flexibility** | Hot-swappable planning & coding models | Choose the right model for each task |
| ğŸ“Š **Resource Optimization** | Intelligent memory management | Smooth operation on any system |

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
git clone https://github.com/Shawn5cents/Ncode
cd Ncode
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Download required models:
```bash
# Create models directory
mkdir -p models

# Download recommended models
# Planning: Mistral-7B-Instruct
# Coding: CodeLlama-7B-Instruct
```

## ğŸš€ Usage

Start generating code with our intuitive CLI:
```bash
python backend/cli_client.py
```

### ğŸ® Commands

| Command | Description |
|---------|-------------|
| `mtp` | ğŸš„ Toggle Multi-Token Prediction mode |
| `models` | ğŸ“‹ List available models |
| `switch TYPE MODEL` | ğŸ”„ Change active model (TYPE: planning\|coding) |
| `help` | ğŸ’¡ Show help message |
| `quit` | ğŸ‘‹ Exit program |

### ğŸ’« Example: Creating a Web Server

```python
> create a fast http server with rate limiting

[Planning] Designing architecture...
âœ“ Selected FastAPI framework
âœ“ Added rate limiting middleware
âœ“ Included error handling

[Coding] Implementing solution...

from fastapi import FastAPI, Request
from fastapi.middleware.throttling import ThrottlingMiddleware
import uvicorn

app = FastAPI(title="Fast HTTP Server")

# Rate limiting middleware
app.add_middleware(
    ThrottlingMiddleware,
    rate_limit=100,  # requests
    time_window=60   # seconds
)

@app.get("/")
async def root():
    return {"message": "Welcome to the rate-limited server!"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

âœ¨ Generated server.py with rate limiting!
```

## ğŸ—ï¸ Architecture

Our dual-model architecture ensures both speed and quality:

- ğŸ§  **LocalModelClient**: Smart engine managing model operations
- âš¡ **Parallel Generation**: Asynchronous planning and implementation
- ğŸš„ **MTP Mode**: Experimental single-model generation
- ğŸ¯ **GPU Optimization**: Automatic layer configuration
- ğŸ“Š **Resource Management**: Dynamic context and cleanup

## ğŸ”§ Technical Details

- ğŸ® GPU memory-based optimization
- ğŸ”’ Thread-safe model loading
- ğŸ“¡ Streaming token generation
- ğŸ§¹ Proper resource cleanup
- ğŸ›¡ï¸ Error handling with CPU fallback

## ğŸ‘¥ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ™ Acknowledgments

### Special Thanks
- **michael5cents**: For inspiration and invaluable support
- **Louisce5cents**: For crucial support in shaping the project

### Project Inspirations
- [Aider](https://github.com/paul-gauthier/aider): Inspiration for architect mode
- [DeepSeek](https://github.com/deepseek-ai/DeepSeek-Coder): Inspiration for MTP

See [CREDITS.md](docs/CREDITS.md) for a complete list of acknowledgments.

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

<div align="center">
Made with â¤ï¸ by the Ncode Team
</div>
